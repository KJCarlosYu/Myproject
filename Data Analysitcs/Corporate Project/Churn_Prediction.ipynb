{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxv55FdWpm2H",
        "outputId": "5707c240-0268-4548-bd96-92200ebf9da5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcpQB326FgtP"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DfFQT05p8Mo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, recall_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNt3Un-Lve5j"
      },
      "source": [
        "# Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLmHycfjEFw0"
      },
      "outputs": [],
      "source": [
        "data = pd.read_parquet('/content/drive/MyDrive/IE/Capstone/transactions_dataset.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SGwV1texZ-lY",
        "outputId": "155009d5-fa9e-462a-dc7f-5061359f5e77"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAVcDCAeZBO7"
      },
      "source": [
        "# Churn Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld_uSCaZYU0G"
      },
      "source": [
        "In this section, we explore the definition of churn.\n",
        "\n",
        "We came up with 5 approaches:\n",
        "* Confidence interval\n",
        "* K-means\n",
        "* Confidence interval + K-means\n",
        "* K-means + purchase interval\n",
        "* Windows (applied solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8TCllWOOa7o"
      },
      "source": [
        "## Confidence interval calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psiY7k_UaoUh"
      },
      "source": [
        "The primary concept behind using a confidence interval is to determine where the recency of each client's purchase falls on a normal distribution curve. If the purchase intervals of clients follow a normal distribution, we can calculate the position of the recency within this curve. When the recency lies beyond the right tail of the 95% confidence interval, it indicates that the likelihood of such a recency occurring is less than 2.5%. This suggests that the customer has most likely churned.\n",
        "\n",
        "However, after conducting a normality test on the purchase behaviour of our customers, we found that it does not follow a normal distribution. As a result, we cannot apply this approach to assess customer churn accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duq_pkBeLEbF"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data)\n",
        "df['date_order'] = pd.to_datetime(df['date_order'])\n",
        "\n",
        "# keep 1 each day\n",
        "df = df.drop_duplicates(subset=['client_id', 'date_order'])\n",
        "\n",
        "# order by client_id and date_order\n",
        "df = df.sort_values(by=['client_id', 'date_order'])\n",
        "\n",
        "# calculate interval\n",
        "df['purchase_interval'] = df.groupby('client_id')['date_order'].diff().dt.days\n",
        "\n",
        "# delete null\n",
        "purchase_intervals = df.dropna(subset=['purchase_interval'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "QDchMe5iMed_",
        "outputId": "2b239e42-4d1b-4fc6-9e89-1a5a4bcc8b61"
      },
      "outputs": [],
      "source": [
        "purchase_intervals.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGIYGmNJOfzi",
        "outputId": "4082eef9-44c7-44b1-ff02-5ab2b66b1583"
      },
      "outputs": [],
      "source": [
        "# Function to check normality for individual customers\n",
        "def check_customer_normality(df, client_id):\n",
        "    customer_data = df[df['client_id'] == client_id]['purchase_interval']\n",
        "    if len(customer_data) < 3:  # Shapiro-Wilk test requires at least 3 data points\n",
        "        return np.nan, np.nan\n",
        "    shapiro_test = stats.shapiro(customer_data)\n",
        "    return shapiro_test.statistic, shapiro_test.pvalue\n",
        "\n",
        "# Apply this function to each customer\n",
        "results = []\n",
        "for client in purchase_intervals['client_id'].unique():\n",
        "    statistic, pvalue = check_customer_normality(purchase_intervals, client)\n",
        "    results.append({'client_id': client, 'statistic': statistic, 'pvalue': pvalue})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# proportion\n",
        "alpha = 0.05\n",
        "results_df['is_normal'] = results_df['pvalue'].apply(lambda p: p > alpha)\n",
        "normal_customers = results_df['is_normal'].sum()\n",
        "total_customers = results_df['client_id'].nunique()\n",
        "normal_ratio = normal_customers / total_customers\n",
        "\n",
        "print(f\"Number of customers following normal distribution: {normal_customers}\")\n",
        "print(f\"Total number of customers: {total_customers}\")\n",
        "print(f\"Proportion of customers following normal distribution: {normal_ratio:.2f}\")\n",
        "\n",
        "# result\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPSfxKGcnfI3"
      },
      "source": [
        "## K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t92G8dpjdqa8"
      },
      "source": [
        "K-means is able to cluster the customers according to their puchase behaviour, so by using k-means, we might be able to find a cluster that implies high probability of churn.\n",
        "\n",
        "Though we did find a cluster with extremely high recency, the cluster also shows characteristics of low clv and low frequency, which means that those customers in the cluster are the ones that have purchased only once or twice, so we were not able to detect the churn of high value customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVk5ld2imQo"
      },
      "source": [
        "### Finding the best cluster number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7doVD669nKCh"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/IE/Capstone/client.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym6Fx8eUKWUA"
      },
      "outputs": [],
      "source": [
        "#reseting indexes\n",
        "data=data.set_index('Unnamed: 0',drop=True)\n",
        "data=data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9j6NeOeK1eL",
        "outputId": "802a8f29-e67a-41a3-de20-f64c7dcc016e"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "LI_kq8rHW28t",
        "outputId": "9193bd16-f604-488f-d819-0e294b8458f0"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Select relevant columns\n",
        "features = ['recency', 'frequency', 'monetary', 'avg_days_between_purchases', 'returns',\n",
        "            'unique_products', 'total_quantity', 'purchase_interval_variance', 'lifespan']\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=features)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "\n",
        "### Step 3: K-means Clustering\n",
        "\n",
        "# Determine the optimal number of clusters using the elbow method\n",
        "inertia = []\n",
        "range_n_clusters = range(1, 13)\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    kmeans.fit(scaled_features)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range_n_clusters, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal Number of Clusters')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt66NYOBFpSI"
      },
      "source": [
        "### Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuMFnqRUFt_0"
      },
      "source": [
        "The clustering analysis is done in `Dataiku`, the following is just an example showing how the clusters might look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "wAZehN1DBZfa",
        "outputId": "8d2d900a-4df3-4e11-fca6-62a48bb41860"
      },
      "outputs": [],
      "source": [
        "# Fit K-means with the optimal number of clusters\n",
        "optimal_n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=0)\n",
        "df['cluster'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "### Step 4: Cluster Analysis\n",
        "\n",
        "# Add cluster labels to the original DataFrame\n",
        "df['cluster'] = kmeans.labels_\n",
        "\n",
        "# Analyze clusters\n",
        "cluster_summary = df.groupby('cluster')[features].mean()\n",
        "\n",
        "# Check the number of clients in each cluster\n",
        "cluster_counts = df['cluster'].value_counts()\n",
        "\n",
        "# Plot cluster distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='cluster')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Number of Clients')\n",
        "plt.title('Number of Clients in Each Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89RfZTO1eNA9"
      },
      "source": [
        "## K-means + Confidence Interval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x4Pw2pJje23"
      },
      "source": [
        "In order to detect the churn status of the customers with high value, we tried to **conduct the confidence interval within the high value clusters**. However, those clusters did not pass through the normality test and the approach was abandoned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwIJnsYnOAfo"
      },
      "outputs": [],
      "source": [
        "clustered = pd.read_csv('/content/drive/MyDrive/IE/Capstone/client_correct_clustered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "jcTASBk2j-hX",
        "outputId": "2ce364ba-e33a-4903-c5f7-125babf12e64"
      },
      "outputs": [],
      "source": [
        "clustered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myYRjrwbkTks",
        "outputId": "90bf836e-354a-4730-ea74-30dfd64b1490"
      },
      "outputs": [],
      "source": [
        "# cluster_0: tier_3; cluster_1: churned; cluster_2: tier_1; cluster_3: at_risk; cluster_4: tier_2\n",
        "clustered['cluster_labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGPr7-TFgnvc"
      },
      "source": [
        "#### Normality test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm1U2cslqMVs"
      },
      "outputs": [],
      "source": [
        "data = pd.read_parquet('/content/drive/MyDrive/IE/Capstone/transactions_dataset.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vpxfjqyngpzf",
        "outputId": "19693348-f06d-4c25-ca3c-57a43d44b99f"
      },
      "outputs": [],
      "source": [
        "# Initialize a DataFrame with your data\n",
        "df = data\n",
        "df['date_order'] = pd.to_datetime(df['date_order'])\n",
        "\n",
        "# Keep only one transaction per client per day\n",
        "df = df.drop_duplicates(subset=['client_id', 'date_order'])\n",
        "\n",
        "# Order by client_id and date_order\n",
        "df = df.sort_values(by=['client_id', 'date_order'])\n",
        "\n",
        "# Calculate purchase intervals\n",
        "df['purchase_interval'] = df.groupby('client_id')['date_order'].diff().dt.days\n",
        "\n",
        "# Delete rows with null purchase_interval\n",
        "purchase_intervals = df.dropna(subset=['purchase_interval'])\n",
        "\n",
        "# Merge with clustered DataFrame\n",
        "clustered = pd.merge(purchase_intervals, clustered[['client_id', 'cluster_labels']], on='client_id')\n",
        "\n",
        "# Ensure 'purchase_interval' column has no NaN values after merging\n",
        "clustered = clustered.dropna(subset=['purchase_interval'])\n",
        "\n",
        "# Function to check normality for individual customers\n",
        "def check_customer_normality(df, client_id):\n",
        "    customer_data = df[df['client_id'] == client_id]['purchase_interval'].dropna()\n",
        "    if len(customer_data) < 3:  # Shapiro-Wilk test requires at least 3 data points\n",
        "        return np.nan, np.nan\n",
        "    shapiro_test = stats.shapiro(customer_data)\n",
        "    return shapiro_test.statistic, shapiro_test.pvalue\n",
        "\n",
        "# Initialize a list to hold results for all specified clusters\n",
        "results = []\n",
        "\n",
        "# List of clusters to analyze\n",
        "clusters_to_analyze = ['cluster_1', 'cluster_3', 'cluster_4']\n",
        "\n",
        "for cluster in clusters_to_analyze:\n",
        "    # Filter the dataframe for the current cluster\n",
        "    cluster_df = clustered[clustered['cluster_labels'] == cluster]\n",
        "\n",
        "    # Apply normality test to each customer in the current cluster\n",
        "    for client in cluster_df['client_id'].unique():\n",
        "        statistic, pvalue = check_customer_normality(cluster_df, client)\n",
        "        results.append({'client_id': client, 'cluster_label': cluster, 'statistic': statistic, 'pvalue': pvalue})\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Calculate the proportion of customers following a normal distribution for each cluster\n",
        "alpha = 0.05\n",
        "results_df['is_normal'] = results_df['pvalue'].apply(lambda p: p > alpha)\n",
        "\n",
        "# Proportion of normal customers per cluster\n",
        "for cluster in clusters_to_analyze:\n",
        "    cluster_results = results_df[results_df['cluster_label'] == cluster]\n",
        "    normal_customers = cluster_results['is_normal'].sum()\n",
        "    total_customers = cluster_results['client_id'].nunique()\n",
        "    normal_ratio = normal_customers / total_customers if total_customers > 0 else 0\n",
        "    print(f\"Cluster {cluster} - Number of customers following normal distribution: {normal_customers}\")\n",
        "    print(f\"Cluster {cluster} - Total number of customers: {total_customers}\")\n",
        "    print(f\"Cluster {cluster} - Proportion of customers following normal distribution: {normal_ratio:.2f}\")\n",
        "\n",
        "# Print overall result\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjindnR2xwpS"
      },
      "source": [
        "## K-means + Average purchasing days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xy4n7VRkDYq"
      },
      "source": [
        "Knowing that confidence interval is not available in this case, we tried defining churn with K-means and average purchasing days.\n",
        "\n",
        "The approach is as follows:\n",
        "- apply churned for customers belonging to the cluster with extremely high recency and CLV.\n",
        "- apply churned for customers that recency is 3 times larger than their average purchasing day.\n",
        "\n",
        "This approach is able to customise the definition of churn according to different customers and also consider the customer value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UANoxmASz1DE"
      },
      "outputs": [],
      "source": [
        "clustered = pd.read_csv('/content/drive/MyDrive/IE/Capstone/client_correct_clustered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nLZBwX0ugB",
        "outputId": "c3b28ad9-f809-4a62-e336-a3a63ac83d07"
      },
      "outputs": [],
      "source": [
        "clustered.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvywA7vCx-Cp",
        "outputId": "2a0b9a8a-292c-4207-e5f6-2cb4fdffbf49"
      },
      "outputs": [],
      "source": [
        "# Set a threshold multiplier. If recency exceeds 3 times the average purchase interval, the customer is at risk of churn\n",
        "threshold_multiplier = 3\n",
        "\n",
        "# Define churn risk\n",
        "clustered['churn_risk'] = clustered['recency'] > (threshold_multiplier * clustered['avg_days_between_purchases'])\n",
        "\n",
        "# Print results\n",
        "print(clustered[['client_id', 'recency', 'avg_days_between_purchases', 'churn_risk']])\n",
        "\n",
        "# Count the number of customers at risk of churn\n",
        "churn_customers = clustered['churn_risk'].sum()\n",
        "total_customers = clustered['client_id'].nunique()\n",
        "churn_ratio = churn_customers / total_customers\n",
        "\n",
        "print(f\"Number of customers at risk of churn: {churn_customers}\")\n",
        "print(f\"Total number of customers: {total_customers}\")\n",
        "print(f\"Proportion of customers at risk of churn: {churn_ratio:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s81UnUxRxD5",
        "outputId": "312ee053-27c3-4adf-ee7c-750557eb1324"
      },
      "outputs": [],
      "source": [
        "# Create the 'churn' column based on the condition\n",
        "clustered.loc[clustered['cluster_labels']=='cluster_0','churn_risk'] = True\n",
        "\n",
        "clustered.reset_index(drop=True,inplace=True)\n",
        "clustered.head()\n",
        "\n",
        "# Count the number of customers at risk of churn\n",
        "churn_customers = clustered['churn_risk'].sum()\n",
        "total_customers = clustered['client_id'].nunique()\n",
        "churn_ratio = churn_customers / total_customers\n",
        "\n",
        "print(f\"Number of customers at risk of churn: {churn_customers}\")\n",
        "print(f\"Total number of customers: {total_customers}\")\n",
        "print(f\"Proportion of customers at risk of churn: {churn_ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "1vMY_iaJ2NWk",
        "outputId": "4b7be019-a0db-4b1e-997e-f6343444430c"
      },
      "outputs": [],
      "source": [
        "clustered[clustered['cluster_labels']=='cluster_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihejAkJj1VfZ",
        "outputId": "e08a0bbf-5139-4fda-c708-d850393a77e3"
      },
      "outputs": [],
      "source": [
        "churn_customers = clustered.loc[clustered['cluster_labels']=='cluster_4','churn_risk'].sum()\n",
        "total_customers = clustered[clustered['cluster_labels']=='cluster_4']['client_id'].count()\n",
        "churn_ratio = churn_customers / total_customers\n",
        "print(f\"Number of customers at risk of churn: {churn_customers}\")\n",
        "print(f\"Total number of customers: {total_customers}\")\n",
        "print(f\"Proportion of customers at risk of churn: {churn_ratio:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHHJLG_0oG0F"
      },
      "source": [
        "## Windows approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF6VmHm3oKc2"
      },
      "source": [
        "Though the K-means + average purchasing days approach can define churn differently according to the customer purchase behaviour, it is possible to cause data leakage when training a model. Therefore, we decided to use the window approach for the model.\n",
        "\n",
        "The idea is seperating the dataset into different windows, we used 2 months as the time period in this case. Then we assign the value of 1, if the customer has purchased during the 2 months, and 0, if the has not purchased. We obtained a new dataframe containing the customer behaviour and we're going to use it for prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXcQxbM2pYb6",
        "outputId": "740cdf72-5e17-4f08-9f8c-b6aef9aceaea"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "data = pd.read_parquet('/content/drive/MyDrive/IE/Capstone/transactions_dataset.parquet')\n",
        "\n",
        "# datetime\n",
        "data['date_order'] = pd.to_datetime(data['date_order'])\n",
        "\n",
        "# dataset for frequency calculation\n",
        "data_unique = data.drop_duplicates(subset=['client_id', 'date_order'])\n",
        "\n",
        "# generate 2-month window, starting from 0\n",
        "min_date = data_unique['date_order'].min()\n",
        "data_unique['window'] = ((data_unique['date_order'].dt.year - min_date.year) * 6 +\n",
        "                         (data_unique['date_order'].dt.month - min_date.month) // 2)\n",
        "\n",
        "# generate client_behaviour\n",
        "client_behaviour = data_unique.groupby(['client_id', 'window']).size().unstack(fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "ZtyC5lo4chtV",
        "outputId": "c4efa9d2-707e-4e28-8500-e1d4492863b8"
      },
      "outputs": [],
      "source": [
        "client_behaviour.columns = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "# define purchase behaviour\n",
        "def process_window_12(value):\n",
        "    return 0 if value == 0 else 1\n",
        "\n",
        "# assign values to each of the columns\n",
        "for column in client_behaviour.columns:\n",
        "    client_behaviour[column] = client_behaviour[column].apply(process_window_12)\n",
        "client_behaviour.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAbOjUvUcnn1",
        "outputId": "fca2f91b-1673-4611-8e15-2172094d8024"
      },
      "outputs": [],
      "source": [
        "client_behaviour[12].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhmLvvwSp_Si"
      },
      "outputs": [],
      "source": [
        "client_behaviour = pd.read_csv( '/content/drive/MyDrive/IE/Capstone/client_behaviour.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fWIt2ITl1A2"
      },
      "source": [
        "# Churn Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE544-r5G48Y"
      },
      "source": [
        "To decrease the churn rate of the customers and increase customer lifetime value, we developed a churn model based on the window dataset that we generated for customer behaviour analysis.\n",
        "\n",
        "We tried 2 approaches:\n",
        "- We created a dataset without the transactions happening in the lastest 30 days, and generated a client dataset based on it. The churn status is calculated in both datasets (with and without the latest 30 days transactions), and we tried to predict the changes of churn status.\n",
        "- We used the purchased-or-not window dataset to predict whether a customer is going to purchase or not in the next two months."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT102f11LlAY"
      },
      "source": [
        "## First Approach - based on client dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAzDFXRCLqoO"
      },
      "source": [
        "As mentioned,in the first approach, we tried using a client dataset without the transactions happening in the latest 30 days. Though without the 30 days of transactions, the RFM, CLV and other values will change accordingly and prevent data leakage, the definition of churn here is rather robust and hard to change over time, which results in the merely 18% of the changes of status during the one month period.\n",
        "\n",
        "Additionally, using a summary of the whole history for prediction is not actually tracking customer behaviour evolution, for which we developed the second approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJQeLQH3T-ur"
      },
      "source": [
        "### Dataset without latest 30 days' transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAncleLaU-im",
        "outputId": "b1b1ed81-3bbc-46b4-c3dc-fd7a621fd37f"
      },
      "outputs": [],
      "source": [
        "# Converting datetime format\n",
        "data = pd.read_parquet('/content/drive/MyDrive/IE/Capstone/transactions_dataset.parquet')\n",
        "data['date_order'] = pd.to_datetime(data['date_order'])\n",
        "data['date_invoice'] = pd.to_datetime(data['date_invoice'])\n",
        "\n",
        "# Determine the latest date in the data\n",
        "latest_date_order = data['date_order'].max()\n",
        "\n",
        "# Calculate the cutoff date\n",
        "cutoff_date = latest_date_order - pd.Timedelta(days=30)\n",
        "\n",
        "# Filter the data to exclude transactions from the last 30 days\n",
        "data = data[data['date_order'] < cutoff_date]\n",
        "\n",
        "# Display the filtered data\n",
        "print(data.head())\n",
        "\n",
        "# Save the filtered dataset if needed\n",
        "# filtered_data.to_parquet('/path/to/save/filtered_data.parquet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_vxvYkRWmSw"
      },
      "source": [
        "### Client profile for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQundukHWsAr",
        "outputId": "0ce921b7-3eb6-4b58-82e3-a696934314fe"
      },
      "outputs": [],
      "source": [
        "# Latest date in the dataset\n",
        "latest_date = data['date_order'].max()\n",
        "\n",
        "# Drop duplicate transactions per customer per day\n",
        "unique_transactions = data.drop_duplicates(subset=['client_id', 'date_order'])\n",
        "\n",
        "# Aggregate the data to calculate RFM metrics\n",
        "rfm = data.groupby('client_id').agg({\n",
        "    'date_order': lambda x: (latest_date - x.max()).days,\n",
        "    'sales_net': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate frequency from unique_transactions\n",
        "frequency = unique_transactions.groupby('client_id')['date_order'].count()\n",
        "\n",
        "# Merge frequency into the RFM dataframe\n",
        "rfm = rfm.merge(frequency, on='client_id')\n",
        "\n",
        "# Rename columns for clarity\n",
        "rfm = rfm.rename(columns={\n",
        "    'date_order': 'recency',\n",
        "    'sales_net': 'monetary'\n",
        "})\n",
        "df = data.drop_duplicates(subset=['client_id', 'date_order'])\n",
        "df = df.sort_values(by=['client_id', 'date_order'])\n",
        "\n",
        "# calculate interval\n",
        "df['purchase_interval'] = df.groupby('client_id')['date_order'].diff().dt.days\n",
        "avg_days_between_purchases = df.groupby('client_id')['purchase_interval'].mean().reset_index(name='avg_days_between_purchases')\n",
        "variance_df = df.groupby('client_id')['purchase_interval'].var().reset_index(name='purchase_interval_variance')\n",
        "variance_df['purchase_interval_variance'] = variance_df['purchase_interval_variance'].fillna(0)\n",
        "\n",
        "# Replace NaN with 0 for customers with only one purchase\n",
        "most_common_channel = data.groupby('client_id')['order_channel'].agg(lambda x: x.mode()[0]).reset_index(name='most_common_order_channel')\n",
        "\n",
        "# Most Common Order Channel\n",
        "most_common_channel = data.groupby('client_id')['order_channel'].agg(lambda x: x.mode()[0]).reset_index(name='most_common_order_channel')\n",
        "\n",
        "# Number of returns\n",
        "returns = data[data['sales_net'] < 0].groupby('client_id').size().reset_index(name='returns')\n",
        "returns['returns'] = returns['returns'].fillna(0)\n",
        "\n",
        "# Most Purchased Product\n",
        "most_purchased_product = data.groupby(['client_id', 'product_id']).size().reset_index(name='count')\n",
        "most_purchased_product = most_purchased_product.loc[most_purchased_product.groupby('client_id')['count'].idxmax()].drop(columns='count')\n",
        "\n",
        "# Number of Unique Products Purchased\n",
        "unique_products = data.groupby('client_id')['product_id'].nunique().reset_index(name='unique_products')\n",
        "\n",
        "# Brach\n",
        "most_common_branch = data.groupby('client_id')['branch_id'].agg(lambda x: x.mode()[0]).reset_index(name='most_common_branch')\n",
        "\n",
        "# total quantity\n",
        "total_quantity = data.groupby('client_id')['quantity'].sum().reset_index(name='total_quantity')\n",
        "\n",
        "# First Purchase date\n",
        "first_purchase_date = data.groupby('client_id')['date_order'].min().reset_index(name='first_purchase_date')\n",
        "\n",
        "last_purchase_date = data.groupby('client_id')['date_order'].max().reset_index(name='last_purchase_date')\n",
        "last_invoice_date = data.groupby('client_id')['date_invoice'].max().reset_index(name='last_invoice_date')\n",
        "#CLV\n",
        "# value\n",
        "avg_purchase_value = data.groupby('client_id')['sales_net'].mean()\n",
        "# lifespan\n",
        "customer_lifespan = (data.groupby('client_id')['date_order'].max() - data.groupby('client_id')['date_order'].min()).dt.days / 365\n",
        "# calculation\n",
        "clv = avg_purchase_value * frequency * customer_lifespan\n",
        "\n",
        "clv_df = pd.DataFrame({'client_id': clv.index, 'CLV': clv.values})\n",
        "print(clv_df.head())\n",
        "final_df = first_purchase_date \\\n",
        "    .merge(last_purchase_date, on='client_id') \\\n",
        "    .merge(last_invoice_date, on='client_id')\\\n",
        "    .merge(rfm, on='client_id') \\\n",
        "    .merge(avg_days_between_purchases, on='client_id') \\\n",
        "    .merge(most_common_channel, on='client_id') \\\n",
        "    .merge(returns, on='client_id', how='left') \\\n",
        "    .merge(most_purchased_product, on='client_id') \\\n",
        "    .merge(unique_products, on='client_id') \\\n",
        "    .merge(most_common_branch, on='client_id') \\\n",
        "    .merge(total_quantity, on='client_id') \\\n",
        "    .merge(variance_df, on='client_id')\\\n",
        "    .merge(customer_lifespan, on='client_id')\\\n",
        "    .merge(clv_df, on='client_id')\n",
        "\n",
        "final_df=final_df.rename(columns={'date_order':'lifespan','date_order_x':'recency','date_order_y':'frequency'})\n",
        "# Fill NA values in returns with 0\n",
        "final_df['returns'] = final_df['returns'].fillna(0)\n",
        "final_df['avg_days_between_purchases'] = final_df['avg_days_between_purchases'].fillna(0)\n",
        "\n",
        "# Display the final DataFrame\n",
        "final_df.head()\n",
        "\n",
        "final_df.to_csv('/content/drive/MyDrive/IE/Capstone/client_prediction.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3zxfU7BX_04"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXG1isFzS2rF"
      },
      "outputs": [],
      "source": [
        "final_df = pd.read_csv('/content/drive/MyDrive/IE/Capstone/client_prediction.csv')\n",
        "clustered = pd.read_csv('/content/drive/MyDrive/IE/Capstone/client_correct_clustered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "rDAKNJVTYPSe",
        "outputId": "95fca3c9-763a-4d4a-c0fd-e017ff321764"
      },
      "outputs": [],
      "source": [
        "clustered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lj4bVYoYBsK"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.merge(clustered[['client_id','cluster_labels']],on='client_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCOZA2WXcNqv",
        "outputId": "6a3a4738-e08c-4c94-88fd-f4674dc9f021"
      },
      "outputs": [],
      "source": [
        "# Set a threshold multiplier. If recency exceeds 3 times the average purchase interval, the customer is at risk of churn\n",
        "threshold_multiplier = 3\n",
        "\n",
        "# Define churn risk\n",
        "final_df['churn_risk'] = final_df['recency'] > (threshold_multiplier * final_df['avg_days_between_purchases'])\n",
        "\n",
        "# Print results\n",
        "print(final_df[['client_id', 'recency', 'avg_days_between_purchases', 'churn_risk']])\n",
        "\n",
        "# Count the number of customers at risk of churn\n",
        "churn_customers = final_df['churn_risk'].sum()\n",
        "total_customers = final_df['client_id'].nunique()\n",
        "churn_ratio = churn_customers / total_customers\n",
        "\n",
        "print(f\"Number of customers at risk of churn: {churn_customers}\")\n",
        "print(f\"Total number of customers: {total_customers}\")\n",
        "print(f\"Proportion of customers at risk of churn: {churn_ratio:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l16I1a5PUqrh",
        "outputId": "e3d989f5-9915-4a35-fdbb-c5620f168bdc"
      },
      "outputs": [],
      "source": [
        "# Set a threshold multiplier. If recency exceeds 3 times the average purchase interval, the customer is at risk of churn\n",
        "threshold_multiplier = 3\n",
        "\n",
        "# Define churn risk\n",
        "clustered['churn_risk'] = clustered['recency'] > (threshold_multiplier * clustered['avg_days_between_purchases'])\n",
        "\n",
        "# Print results\n",
        "print(clustered[['client_id', 'recency', 'avg_days_between_purchases', 'churn_risk']])\n",
        "\n",
        "# Count the number of customers at risk of churn\n",
        "churn_customers = clustered['churn_risk'].sum()\n",
        "total_customers = clustered['client_id'].nunique()\n",
        "churn_ratio = churn_customers / total_customers\n",
        "\n",
        "print(f\"Number of customers at risk of churn: {churn_customers}\")\n",
        "print(f\"Total number of customers: {total_customers}\")\n",
        "print(f\"Proportion of customers at risk of churn: {churn_ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "HWOhNabhUnou",
        "outputId": "4cb17504-67be-4040-8f17-afed78851ea3"
      },
      "outputs": [],
      "source": [
        "final_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpKoBlIrdFCi"
      },
      "outputs": [],
      "source": [
        "# Adding the result after 30 days\n",
        "final_df = final_df.merge(clustered[['client_id','churn_risk']],on='client_id')\n",
        "final_df.rename(columns={'churn_risk_y':'churn_result','churn_risk_x':'churn_status'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "KA45QIZgQKjS",
        "outputId": "172c45ee-6b08-4995-9772-fe75c8f9da46"
      },
      "outputs": [],
      "source": [
        "final_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNNQrsIgQEUM"
      },
      "source": [
        "### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXMhVMD2qlwt",
        "outputId": "42a697b3-f6d5-4d06-83a2-fb2525dd13dc"
      },
      "outputs": [],
      "source": [
        "pip install xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CwO56PnOfIzU",
        "outputId": "9b4830dc-0ee6-4982-c0e5-d903a3a98365"
      },
      "outputs": [],
      "source": [
        "clustered['last_purchase_date'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jBjaJS_-egat",
        "outputId": "08f32377-d4d3-40b1-fe2c-a5958f895a57"
      },
      "outputs": [],
      "source": [
        "final_df['last_purchase_date'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDeOqElzl306",
        "outputId": "e040bf1e-1f19-462b-950b-6bb07e997188"
      },
      "outputs": [],
      "source": [
        "# Exclude 'cluster_labels' and other non-predictive columns\n",
        "X = final_df.drop(columns=['client_id', 'cluster_labels', 'churn_result', 'first_purchase_date', 'last_purchase_date', 'last_invoice_date','product_id','most_common_branch'])\n",
        "y = final_df['churn_result']\n",
        "\n",
        "# Encode categorical variables\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[column] = le.fit_transform(X[column])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Print shapes for verification\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwgpS1FFrIpn"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[column] = le.fit_transform(X[column])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWF5jwdXspxi",
        "outputId": "4c2e3194-8261-4d9a-95ca-bd727792be83"
      },
      "outputs": [],
      "source": [
        "X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNH0tZmGs68l",
        "outputId": "c5cdc109-d6ec-4d94-b2df-6c3662ab3b31"
      },
      "outputs": [],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "I6_G4ZF4raxA",
        "outputId": "294cb1f6-2d22-4c20-f7fe-c645a9ae4e57"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights to handle class imbalance\n",
        "neg, pos = np.bincount(y_train)\n",
        "scale_pos_weight = neg / pos\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight)\n",
        "\n",
        "# Define the parameter grid for random search\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 300, 50),\n",
        "    'max_depth': np.arange(3, 10, 1),\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'min_child_weight': [1, 2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,  # Number of parameter settings sampled\n",
        "    scoring='roc_auc',  # Use recall as the scoring metric\n",
        "    cv=4,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "best_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight)\n",
        "best_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i80xr40BXpy",
        "outputId": "fc922dfc-19f8-4586-821e-b12d84f429c3"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Adjust threshold to favor recall\n",
        "threshold = 0.3  # Lower the threshold to favor recall\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "# Evaluate the XGBoost model\n",
        "print(\"XGBoost Model Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.2f}\")\n",
        "\n",
        "# Calculate accuracy for the XGBoost model\n",
        "xgboost_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"XGBoost Model Accuracy: {xgboost_accuracy:.2f}\")\n",
        "\n",
        "# Calculate AUC-ROC for the XGBoost model\n",
        "xgboost_auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"XGBoost Model AUC-ROC: {xgboost_auc_roc:.2f}\")\n",
        "\n",
        "# Calculate Recall for the XGBoost model\n",
        "xgboost_recall = recall_score(y_test, y_pred)\n",
        "print(f\"XGBoost Model Recall: {xgboost_recall:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KasQhRzzhAyW",
        "outputId": "163efcf5-a19b-4620-e882-5818eaa31fee"
      },
      "outputs": [],
      "source": [
        "# Baseline predictions: predict no change in churn status\n",
        "y_baseline_pred = X_test['churn_status'].copy()\n",
        "\n",
        "# Calculate accuracy for baseline model\n",
        "baseline_accuracy = accuracy_score(y_test, y_baseline_pred)\n",
        "\n",
        "# Calculate the number of correct predictions in the baseline model\n",
        "same = (y_test == y_baseline_pred).sum()\n",
        "total = len(y_test)\n",
        "baseline_accuracy_manual = same / total\n",
        "\n",
        "print(f\"Baseline Accuracy (using accuracy_score): {baseline_accuracy:.2f}\")\n",
        "print(f\"Baseline Accuracy (manual calculation): {baseline_accuracy_manual:.2f}\")\n",
        "\n",
        "# Calculate other metrics for the baseline model\n",
        "print(\"Baseline Model Classification Report:\")\n",
        "print(classification_report(y_test, y_baseline_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o02aOuJIjU9K",
        "outputId": "6498dda5-546f-4099-8b84-b1b9076aa753"
      },
      "outputs": [],
      "source": [
        "comparison = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],\n",
        "    'Baseline Model': [\n",
        "        baseline_accuracy,\n",
        "        classification_report(y_test, y_baseline_pred, output_dict=True)['weighted avg']['precision'],\n",
        "        classification_report(y_test, y_baseline_pred, output_dict=True)['weighted avg']['recall'],\n",
        "        classification_report(y_test, y_baseline_pred, output_dict=True)['weighted avg']['f1-score'],\n",
        "        'N/A'  # AUC-ROC is not applicable for the baseline model since probabilities are not used\n",
        "    ],\n",
        "    'XGBoost Model': [\n",
        "        xgboost_accuracy,\n",
        "        classification_report(y_test, y_pred, output_dict=True)['weighted avg']['precision'],\n",
        "        classification_report(y_test, y_pred, output_dict=True)['weighted avg']['recall'],\n",
        "        classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score'],\n",
        "        xgboost_auc_roc\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison)\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oSh148PQRIs"
      },
      "source": [
        "The following part is model evaluation based on the important clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqxroPh3kEcu"
      },
      "outputs": [],
      "source": [
        "important_customers = final_df[final_df['cluster_labels'].isin(['cluster_1','cluster_4','cluster_3'])]\n",
        "X_imp = important_customers.drop(columns=['client_id', 'cluster_labels', 'churn_result', 'first_purchase_date', 'last_purchase_date', 'last_invoice_date','product_id','most_common_branch'])\n",
        "y_imp = important_customers['churn_result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH7Et2jgl2_t"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "for column in X_imp.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X_imp[column] = le.fit_transform(X_imp[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGxxRePMlLk1",
        "outputId": "6e614441-7fc5-4792-c2b3-273299b0e359"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = best_model.predict(X_imp)\n",
        "y_pred_proba = best_model.predict_proba(X_imp)[:, 1]\n",
        "\n",
        "# Evaluate the XGBoost model\n",
        "print(\"XGBoost Model Classification Report:\")\n",
        "print(classification_report(y_imp, y_pred))\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_imp, y_pred_proba):.2f}\")\n",
        "\n",
        "# Calculate accuracy for the XGBoost model\n",
        "xgboost_accuracy = accuracy_score(y_imp, y_pred)\n",
        "print(f\"XGBoost Model Accuracy: {xgboost_accuracy:.2f}\")\n",
        "\n",
        "# Calculate AUC-ROC for the XGBoost model\n",
        "xgboost_auc_roc = roc_auc_score(y_imp, y_pred_proba)\n",
        "print(f\"XGBoost Model AUC-ROC: {xgboost_auc_roc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwJxkT_ZmGe2",
        "outputId": "5c7ce5cd-6ff5-47a0-feb7-63b003d614b8"
      },
      "outputs": [],
      "source": [
        "# Baseline predictions: predict no change in churn status\n",
        "y_baseline_pred = X_imp['churn_status'].copy()\n",
        "\n",
        "# Calculate accuracy for baseline model\n",
        "baseline_accuracy = accuracy_score(y_imp, y_baseline_pred)\n",
        "\n",
        "# Calculate the number of correct predictions in the baseline model\n",
        "same = (y_imp == y_baseline_pred).sum()\n",
        "total = len(y_test)\n",
        "baseline_accuracy_manual = same / total\n",
        "\n",
        "print(f\"Baseline Accuracy (using accuracy_score): {baseline_accuracy:.2f}\")\n",
        "print(f\"Baseline Accuracy (manual calculation): {baseline_accuracy_manual:.2f}\")\n",
        "\n",
        "# Calculate other metrics for the baseline model\n",
        "print(\"Baseline Model Classification Report:\")\n",
        "print(classification_report(y_imp, y_baseline_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rm_IwsFmcyw",
        "outputId": "8d85a082-4a22-45fc-a6d6-ca15a323deed"
      },
      "outputs": [],
      "source": [
        "comparison = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],\n",
        "    'Baseline Model': [\n",
        "        baseline_accuracy,\n",
        "        classification_report(y_imp, y_baseline_pred, output_dict=True)['weighted avg']['precision'],\n",
        "        classification_report(y_imp, y_baseline_pred, output_dict=True)['weighted avg']['recall'],\n",
        "        classification_report(y_imp, y_baseline_pred, output_dict=True)['weighted avg']['f1-score'],\n",
        "        'N/A'  # AUC-ROC is not applicable for the baseline model since probabilities are not used\n",
        "    ],\n",
        "    'XGBoost Model': [\n",
        "        xgboost_accuracy,\n",
        "        classification_report(y_imp, y_pred, output_dict=True)['weighted avg']['precision'],\n",
        "        classification_report(y_imp, y_pred, output_dict=True)['weighted avg']['recall'],\n",
        "        classification_report(y_imp, y_pred, output_dict=True)['weighted avg']['f1-score'],\n",
        "        xgboost_auc_roc\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison)\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp-mgi2ytBk3",
        "outputId": "a2475584-2c4d-4c03-a5a1-3e9f665543f6"
      },
      "outputs": [],
      "source": [
        "# Assume y_true and y_pred_proba are given\n",
        "y_true = y_imp\n",
        "y_pred_proba = best_model.predict_proba(X_imp)[:, 1]\n",
        "\n",
        "# Compute deciles\n",
        "deciles = pd.qcut(y_pred_proba, 10, labels=False)\n",
        "df = pd.DataFrame({'decile': deciles, 'true_label': y_true, 'pred_proba': y_pred_proba})\n",
        "\n",
        "# Calculate lift for each decile\n",
        "lift_table = df.groupby('decile').apply(\n",
        "    lambda x: pd.Series({\n",
        "        'num_customers': x.shape[0],\n",
        "        'num_positives': x['true_label'].sum(),\n",
        "        'positive_rate': x['true_label'].mean(),\n",
        "        'lift': x['true_label'].mean() / y_true.mean()\n",
        "    })\n",
        ").reset_index()\n",
        "\n",
        "# Calculate top decile lift\n",
        "top_decile_lift = lift_table.loc[lift_table['decile'] == 9, 'lift'].values[0]\n",
        "\n",
        "print(lift_table)\n",
        "print(f\"Top Decile Lift: {top_decile_lift:.2f}\")\n",
        "\n",
        "# Detailed lift table interpretation\n",
        "for index, row in lift_table.iterrows():\n",
        "    print(f\"Decile {int(row['decile'])}:\")\n",
        "    print(f\"  Number of customers: {row['num_customers']}\")\n",
        "    print(f\"  Number of positives: {row['num_positives']}\")\n",
        "    print(f\"  Positive rate: {row['positive_rate']:.4f}\")\n",
        "    print(f\"  Lift: {row['lift']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwurTbnL_DlK"
      },
      "source": [
        "## Second Approach - Purchase Windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJhFApflO0MY"
      },
      "source": [
        "In order to consider the customer behaviours evolution, we used the window approach to predict whether a customer will churn in the next 2 months."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJKxwcZvPgZG"
      },
      "source": [
        "### Reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7NIUtQH0km-"
      },
      "outputs": [],
      "source": [
        "client_behaviour = pd.read_csv('/content/drive/MyDrive/IE/Capstone/client_behaviour.csv',index_col='client_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "kS4JFPFX0tFJ",
        "outputId": "4446863c-ea4e-4b83-b264-d82770190ad7"
      },
      "outputs": [],
      "source": [
        "client_behaviour.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_nIdrRpP5XN"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_zOYxgPQhlR"
      },
      "source": [
        "Instead of recording the number of purchases made during the windows, we decided to simplify it to bought(1) or not(0) which facilitates our training using rolling windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "rcT-cws56RcR",
        "outputId": "173fdd8c-c2f0-4db2-db4f-1769a876a91d"
      },
      "outputs": [],
      "source": [
        "client_behaviour.columns = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "def process_window_12(value):\n",
        "    return 0 if value == 0 else 1\n",
        "for column in client_behaviour.columns:\n",
        "    client_behaviour[column] = client_behaviour[column].apply(process_window_12)\n",
        "client_behaviour.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMGOpZpattj",
        "outputId": "ea60d91a-14a5-445f-c9ea-580465bc6c50"
      },
      "outputs": [],
      "source": [
        "client_behaviour[12].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBE3WgdfPkxw"
      },
      "source": [
        "### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXe7rn_8HUT5",
        "outputId": "25fdc4a2-ada3-4f91-9e4b-3d29a6e75bb5"
      },
      "outputs": [],
      "source": [
        "# Initialize result storage\n",
        "results = {\n",
        "    'window': [],\n",
        "    'f1_score': [],\n",
        "    'roc_auc': [],\n",
        "    'accuracy': [],\n",
        "}\n",
        "# Set initial window size and maximum window size\n",
        "initial_window_size = 3\n",
        "max_window_size = client_behaviour.shape[1]\n",
        "\n",
        "# Define parameter distribution for random search\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 300, 50),\n",
        "    'max_depth': np.arange(3, 10, 1),\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'min_child_weight': [1, 2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "feature_importance_df = pd.DataFrame()\n",
        "best_params = None\n",
        "\n",
        "for start_window in range(0, max_window_size - initial_window_size):\n",
        "    train_windows = list(range(0, start_window + initial_window_size))\n",
        "    test_window = start_window + initial_window_size\n",
        "\n",
        "    # Training and testing sets\n",
        "    X_train = client_behaviour[train_windows]\n",
        "    y_train = client_behaviour[test_window]\n",
        "    X_test = client_behaviour[train_windows]\n",
        "    y_test = client_behaviour[test_window]\n",
        "\n",
        "    # Feature engineering\n",
        "    feature_names = [str(i) for i in range(X_train.shape[1])]\n",
        "    X_train.columns = feature_names\n",
        "    X_test.columns = feature_names\n",
        "\n",
        "    # Initialize XGBoost classifier\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "    # Initialize random search\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=50,  # Number of parameter combinations, adjust as needed\n",
        "        scoring='f1',\n",
        "        cv=4,\n",
        "        verbose=2,\n",
        "        random_state=327,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Perform random search\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get best parameters\n",
        "    best_params = random_search.best_params_\n",
        "    print(f\"Best Parameters for window {test_window}: {best_params}\")\n",
        "\n",
        "    # Train final model with best parameters\n",
        "    best_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate model\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results['window'].append(test_window)\n",
        "    results['f1_score'].append(f1)\n",
        "    results['roc_auc'].append(roc_auc)\n",
        "    results['accuracy'].append(accuracy)\n",
        "\n",
        "    # Store feature importance\n",
        "    feature_importances = best_model.feature_importances_\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importances,\n",
        "        'window': test_window\n",
        "    })\n",
        "    feature_importance_df = pd.concat([feature_importance_df, importance_df], axis=0)\n",
        "\n",
        "    # Output results for each iteration\n",
        "    print(f\"Window: {test_window}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc:.2f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    # Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "C8GZKuVKnGOE",
        "outputId": "8491e9f0-2742-4a2a-ae08-61438de29c52"
      },
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh3ol_eDQz8z"
      },
      "source": [
        "As shown, the model suffers a degradation in window 12, we assumed that there should be a large sample difference and the following part justified our assumption. The positive ratio is much lower in the last 2 months and it affected the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSZRsW_dYFNs",
        "outputId": "cacc1ef4-26bc-4508-a36a-66ebd8c74e5a"
      },
      "outputs": [],
      "source": [
        "# checking the distribution of each window\n",
        "for start_window in range(0, max_window_size - initial_window_size):\n",
        "    test_window = start_window + initial_window_size\n",
        "    y_test = client_behaviour[test_window]\n",
        "    positive_ratio = y_test.sum() / len(y_test)\n",
        "    print(f\"Window {test_window}: Positive ratio = {positive_ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alXVBvMeRJaG"
      },
      "source": [
        "Here we're predicting whether the customer will pay or not next 2 months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfLU-MAQWTpJ",
        "outputId": "65e143db-3efa-4d03-8d8f-3e62fa2f4b70"
      },
      "outputs": [],
      "source": [
        "# Prepare input data for window 13 (assuming features from windows 1 to 12)\n",
        "X_window_13 = client_behaviour[list(range(1, 13))]  # Features from windows 1 to 12\n",
        "feature_names_window_13 = [str(i) for i in range(X_window_13.shape[1])]\n",
        "X_window_13.columns = feature_names_window_13\n",
        "\n",
        "# Make predictions for window 13 using the extended model\n",
        "predictions_window_13 = best_model.predict(X_window_13)\n",
        "predictions_proba_window_13 = best_model.predict_proba(X_window_13)[:, 1]\n",
        "\n",
        "# Optionally, you can store predictions_window_13 and predictions_proba_window_13 for further analysis or output.\n",
        "\n",
        "print(f\"Predictions for window 13:\")\n",
        "print(predictions_window_13)\n",
        "print(predictions_proba_window_13)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvn7kuWgRcV4"
      },
      "source": [
        "### Merging the result of predictions with client dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLrz46bQRhxm"
      },
      "source": [
        "In this part, we merge the two datasets to prepare it for the customer segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5tp6jYLIsix",
        "outputId": "2361740a-0747-4a56-d712-d187f110c702"
      },
      "outputs": [],
      "source": [
        "predictions_window_13 = pd.DataFrame(predictions_window_13, columns=[13],index=client_behaviour.index)\n",
        "predictions_window_13.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDxLizmPIDB8"
      },
      "outputs": [],
      "source": [
        "client_behaviour_extended = pd.merge(client_behaviour, predictions_window_13, left_index=True, right_index=True, how='outer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "n4xZHhESKf8B",
        "outputId": "6ec441a2-e939-4509-97c4-bb2127ae7bae"
      },
      "outputs": [],
      "source": [
        "client_behaviour_extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "QjkKKqz7KZRC",
        "outputId": "05ae7599-792b-45c0-ba9b-61dabb64043f"
      },
      "outputs": [],
      "source": [
        "client_dataset = pd.read_csv('/content/drive/MyDrive/IE/Capstone/client.csv',index_col=['client_id'])\n",
        "client_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPxTXurJb7dj"
      },
      "outputs": [],
      "source": [
        "new = client_dataset.merge(client_behaviour_extended[13], left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc6uBCxKcG3b"
      },
      "outputs": [],
      "source": [
        "new.drop(columns ='Unnamed: 0',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgmzFADwcmmR",
        "outputId": "d1302d92-467f-4096-f754-80b6a424c35f"
      },
      "outputs": [],
      "source": [
        "new = new.reset_index()\n",
        "new.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk66ia27dTuz"
      },
      "outputs": [],
      "source": [
        "new.columns = ['client_id','first_purchase_date','last_purchase_date','last_invoice_date','recency','monetary','frequency', 'avg_days_between_purchases','most_common_order_channel','returns',\n",
        "'product_id','unique_products',\n",
        "'most_common_branch','total_quantity',\n",
        "'purchase_interval_variance','lifespan',\n",
        "'CLV','purchase_prediction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7fHv-9KdUqu",
        "outputId": "2df6d41d-991e-4f4e-ce54-66469c304528"
      },
      "outputs": [],
      "source": [
        "new['most_common_order_channel'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRBhpDPwdcGW"
      },
      "outputs": [],
      "source": [
        "new.to_csv('/content/drive/MyDrive/IE/Capstone/client_behaviour_churn.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2FPd8XnRpkE"
      },
      "source": [
        "# Customer Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_h3SjTPRste"
      },
      "source": [
        "In this section, we merged the two datasets churn prediction and client information in order to create different clusters according to different purchase behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "hzWHF9AlUoOv",
        "outputId": "996ed327-f0be-45c6-d7b4-aa911cf6cf98"
      },
      "outputs": [],
      "source": [
        "#import client.csv from my drive\n",
        "import pandas as pd\n",
        "client = pd.read_csv('/content/drive/MyDrive/client_behaviour_churn.csv')\n",
        "client.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvcIVmQIFsXs",
        "outputId": "6975ceb1-c59d-4849-99e4-6bc2ff1c0084"
      },
      "outputs": [],
      "source": [
        "#Summary of the column CLV\n",
        "client['CLV'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1vg3aWYCis2",
        "outputId": "10e1e0b1-9a71-43b0-bee5-2369b20f4e11"
      },
      "outputs": [],
      "source": [
        "#discretize the column CLV into 5, based on its quartiles\n",
        "client['CLV'] = pd.qcut(client['CLV'], q=5, labels=['Very Low', 'Low', 'Medium','High','Very High'])\n",
        "client['CLV'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxi5iWfACwvR",
        "outputId": "013416e1-91f0-4036-f138-93a81cb2fb0f"
      },
      "outputs": [],
      "source": [
        "client['CLV'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "i4nz8OVS7we9",
        "outputId": "c71a3eea-7f87-4434-83f5-d0efa76f8a69"
      },
      "outputs": [],
      "source": [
        "# prompt: Plot a HeatMap of the columns CLV and purchase_prediction\n",
        "# Create a crosstab table of CLV and purchase_prediction\n",
        "ct = pd.crosstab( client['purchase_prediction'],client['CLV'])\n",
        "\n",
        "# Create a heatmap of the crosstab table\n",
        "sns.heatmap(ct, annot=True, cmap=\"YlGnBu\", fmt=\".0f\")\n",
        "plt.ylabel(\"Purchase Prediction\")\n",
        "plt.xlabel(\"CLV\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "8eQi-qxE7we9",
        "outputId": "acf44977-a907-4015-96f0-d4a24e973638"
      },
      "outputs": [],
      "source": [
        "# prompt: Plot a heatmap with the columns CLV and most_common_order_channel\n",
        "# Create a crosstab table of CLV and most_common_order_channel\n",
        "ct = pd.crosstab( client['most_common_order_channel'],client['purchase_prediction'])\n",
        "\n",
        "# Create a heatmap of the crosstab table\n",
        "sns.heatmap(ct, annot=True, cmap=\"YlGnBu\", fmt=\".0f\")\n",
        "plt.ylabel(\"Most Common Order Channel\")\n",
        "plt.xlabel(\"Purchase Prediction\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
